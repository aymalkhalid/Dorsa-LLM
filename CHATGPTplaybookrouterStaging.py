from langchain.chains.router import MultiPromptChain
from langchain.chains import ConversationChain
from langchain.chains.llm import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import AIMessage, HumanMessage, SystemMessage
import os,json
from dotenv import load_dotenv
from flask import Flask,request 
import os

load_dotenv()
#custom module
app = Flask(__name__)

@app.route('/chatgpt_langchain',methods=["POST"])
def chatgpt_langchain():
  request_json=request.get_json()
  ############
  #  This If Block Contains Langchain Router Chain to solve the problem of which use case to trigger based on a question. #
  ############
  if request_json['state'] == "Preference_Card":
    api_source=str(request_json['learning_material'])   
    try: 
      api_source=api_source.replace('{','{{')
      api_source=api_source.replace('}','}}')
    except Exception as e:
       print("Exception:",str(e))       
    question=str(request_json['question'])
    #Item query where the user can ask dorsa if an item is needed in the preference card..
    use_case_1 = """ You are an AI assistant skilled in parsing Dorsa's preference card data. Your specialty lies in handling inquiries regarding items listed on these cards, including whether a specific item features on a particular card or not. If a question surpasses your existing knowledge base, you simply reply with I don't know. \n
    To facilitate comprehension, let's elucidate the structure of the generated response:
    - 'message': This element houses the output generated by the Language Modelling (ChatGPT) algorithm.
    - 'query': This element constitutes the 'count' and 'location' of the queried item.
    - 'count': It exhibits the total presence of the queried item listed on the preference card.
    - 'locations': Constituting an array, it contains indices of the section and item where the queried item is present.
    To illustrate, here's an instance of a detailed expected json Formatted response would be:
    
    {{
      "message": "Output created by the ChatGPT Language Model",
      "query": 
      {{
        "count": total appearances of queried item,
        "locations": [
            {{
                "Section": "Index (starting from 0, then 1, 2, 3, etc.) of the Section that includes the queried item",
                "Item": "Index (starting from 0, then 1, 2, 3, etc.) at which queried item is positioned"
            }},
            {{
                "Section": "Index (starting from 0, then 1, 2, 3, etc.) of the Section that includes the queried item",
                "Item": "Index (starting from 0, then 1, 2, 3, etc.) at which queried item is positioned"
            }}
        ]
      }}
    }}
    Absolutely, maintaining accuracy and truthfulness in responses is paramount. If the assistant does not find the queried item in the available data (e.g., "Image 1"), it should not hallucinate or make up an answer. Rather, it should admit that it doesn't know or that the item was not found. \n
    For example, if "Image 1" does not appear in the provided data and a user asks "Is Image 1 needed?", the response should be: \n
    
    {{
      "message": "I'm sorry, but the data does not contain 'Image 1'. I am unable to determine if it is needed.",
      "query": 
      {{
        "count": 0,
        "locations": []
      }}
    }}
    

    The "count: 0" signifies that the queried item was not found, and the empty "locations" array indicates that there are no indices to report for the item within the dataset. \n
    In this response prototype, the `locations` array signifies the sections wherein the queried item—for instance, "Image 1"—is listed. \n
    """ + """Let's attempt to answer the user question per the described format and context.
    Keeping the provided context in mind answer the User Question:"""+str(api_source)+""" \n 
    User Question: \n
    {input}
    \n
    Remember to only output 'JSON' Formatted response only: \n"""
    #Dictate Section
    use_case_2 = """ You are an AI assistant skilled in parsing Dorsa's preference card data. Your specialty lies in handling inquiries regards to a specific prefence cards, By Identify the index of the preference card and nothing else. If a question surpasses your existing knowledge base, you simply reply with I don't know. Remember to only output  \n
    To facilitate comprehension, let's elucidate the structure of the generated response:
    - 'message': This element should always be 'null'.
    - 'query': This element constitutes the 'count' and 'location' of the queried item.
    - 'count': this element should alway be '1'.
    - 'locations': Constituting an array, it contains indices of the section where the queried item is present while item should always be 'null'.
    To illustrate, here's an instance of a detailed expected response:
    
    {{
      "message": null,
      "query": 
      {{
        "count": 1,
        "locations": [
            {{
                "Section": "Index (starting from 0, then 1, 2, 3, etc.) of the Section that includes the queried item",
                "Item": null
            }}
        ]
      }}
    }}

    \n
    Absolutely, maintaining accuracy and truthfulness in responses is paramount. If the assistant does not find the queried item in the available data (e.g., "Image 1"), it should not hallucinate or make up an answer. Rather, it should admit that it doesn't know or that the item was not found. \n
    For example, if "Section 1" does not appear in the provided data and a user asks "Dictate Section 10?", the json Formatted response would be: \n
    
    {{
      "message": "null",
      "query": 
      {{
        "count": 1,
        "locations": [
            {{
                "Section": null,
                "Item": null
            }}
        ]
      }}
    }}
    
    The "count: 1", message and item must always be null signifies that the queried item was not found, and the null "locations"  indicates that there are no indices to report for the item within the dataset. \n
    In this response prototype, the `locations` array signifies the sections wherein the queried item—for instance, "Image 1"—is listed. \n""" + """Let's attempt to answer the user question per the described format and context.
    Keeping the provided context in mind answer the User Question:"""+str(api_source)+""" \n 
    User Question: \n
    {input}
    \n Remember to only output 'JSON' Formatted response only: \n"""

    #Section Item show..this will generally be in the format of Show <item name> or  Show <item name> from <section name>  or something to that affect
    use_case_3="""You are a proficient AI assistant who specialises in parsing Dorsa's preference card data. Your expertise includes handling requests about displaying specific items from these cards. These requests may ask for a specific item alone (e.g., "Show <item name>") or a specific item from a particular section (e.g., "Show <item name> from <section name>"). If a question surpasses your existing knowledge base, you simply reply with a response indicating that the item or section does not exist.

    The responses that you generate possess a specific structure:

    - 'message': This element should always be 'null'.
    - 'query': This element includes the 'count' and 'location' of the queried item.
    - 'count': This element should always be '1'.
    - 'locations': This is an array containing the position (or index) of the section and the item where the queried item can be found. The 'Section' is the index of the section that includes the queried item, and 'Item' is the position of the queried item in that section.

    For instance, if a user asked, "Show item 2 from section 2", and this item exists in the data, the expected, json Formatted response would be:
    
    {{
      "message": null,
      "query": 
      {{
        "count": 1,
        "locations": [
            {{
                "Section": 1,
                "Item": 1
            }}
        ]
      }}
    }}
    
    Please note, the indices are based on a zero-based system; 'Section': 1 and 'Item': 1 correspond to section 2 and item 2, respectively.

    Credibility and accuracy are necessary. If the AI assistant cannot find the requested item or section in the provided data, it should not invent an answer. Instead, it logically admits that it does not know or that the item or section was not found.

    Remember, the response's '`message`' always being 'null' and 'count' always '1' distinctively identifies this use case. \n""" + """Let's attempt to answer the user question per the described format and context.
    Keeping the provided context in mind answer the User Question:"""+str(api_source)+""" \n 
    User Question: \n
    {input}
    \n Remember to only output 'JSON' Formatted response only: \n"""

    #General Query 
    use_case_4="""
    You are a proficient AI assistant who specialises in handling general queries related to the preference cards that do not follow a specific request formula, enabling you to provide varied information based on user requirements. For example, a user can ask anything related to the preference card, such as its name, items it includes etc. If a question surpasses your existing knowledge base,  you simply reply with I don't know.

    The presented responses demonstrate a distinct structure:
    
    - 'message': This element houses the language modeling response generated by ChatGPT.
    - 'query' This element comprises the 'count' and 'location' which are null.
    - 'count': This element would persist as 'null' for this use case.
    - 'locations': This element persists as 'null' for this use case as it doesn't list specific items or sections from the preference card.

    For instance, if a user asked, "What is the name of the preference card?", the expected response would be a language model generated message about the name of the card.
    json Formatted response would be:
    
    {{
      "message": "Output created by the ChatGPT Language Model",
      "query": 
      {{
                "count": null,
                "location": null
      }}
    }}
    
    Credibility and accuracy are necessary. If the AI assistant cannot find the requested item or section in the provided data, it should not invent an answer. Instead, it logically admits that it does not know.

    Remember, the response's '`message`' always being created by the ChatGPT Language Model. While the 'section' always '1' distinctively identifies this use case.
    """ + """Let's attempt to answer the user question per the described format and context.
    Keeping the provided context in mind answer the User Question:"""+str(api_source)+""" \n 
    User Question: \n
    {input}
    \n Remember to only output 'JSON' Formatted response only: \n
    """
    prompt_infos= [
        {
            "name": "use_case_1",
            "description": "Handles direct queries about whether a specific item is needed or not. Only answer when the exact item name is mentioned.",
            "prompt_template": use_case_1,
        },
        {
            "name": "use_case_2",
            "description": "Dictates the presence of specified sections from Dorsa's preference cards, but does not delve into item details within those sections.",
            "prompt_template": use_case_2,
        },
        {
            "name": "use_case_3",
            "description": "Responds to queries asking to show a specific item or an item from a specified section and provides their occurrences.",
            "prompt_template": use_case_3,
        },
        {
            "name": "use_case_4",
            "description": "Handles general questions about Dorsa's preference cards, but not to provide details regarding specific items or sections.",
            "prompt_template": use_case_4,
        }
    ]

    llm = OpenAI(openai_api_key=os.getenv('OPENAI_API_KEY'),temperature=0)
    #llm = ChatVertexAI(temperature=0)

    destination_chains = {}
    for p_info in prompt_infos:
        name = p_info["name"]
        prompt_template = p_info["prompt_template"]
        prompt = PromptTemplate(template=prompt_template, input_variables=["input"])
        chain = LLMChain(llm=llm, prompt=prompt)
        destination_chains[name] = chain
    default_chain = ConversationChain(llm=llm, output_key="text")

    destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
    destinations_str = "\n".join(destinations)
    router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)
    router_prompt = PromptTemplate(
        template=router_template,
        input_variables=["input"],
        output_parser=RouterOutputParser(),
    )
    router_chain = LLMRouterChain.from_llm(llm, router_prompt)

    chain = MultiPromptChain(
        router_chain=router_chain,
        destination_chains=destination_chains,
        default_chain=default_chain,
        verbose=True,
    )
    chatgpt_result=chain.run(question)
    print(chatgpt_result)
    return chatgpt_result
  ################
  #State is Playbook
  ################
  else: 
    api_source=str(request_json['learning_material'])   
    try: 
      api_source=api_source.replace('{','{{')
      api_source=api_source.replace('}','}}')
    except Exception as e:
       print("Exception:",str(e))       
    question=str(request_json['question'])
    chat = ChatOpenAI(temperature=0, openai_api_key=os.getenv('OPENAI_API_KEY'))
    template = ("""You are Dorsa An AI powered Virtual Assistant In a chaotic health care setting, different medical equipment and implants play cardinal roles. Answering vitally important queries related to their location, quantity, and sizes efficiently is of utmost importance in ensuring smooth patient procedures. \n
              Be confident develop a system where we have a logic installed that any question asked, be it starting with 'how', 'what', 'where'. \n 
              Build an AI-driven system that can deal with queries about the following aspects:
              1. Sparse location of various instruments used during patient care.
              2. Exact quantity available or needed of a particular type of instrument or implant.
              3. Different sizes of orthopedic implants like hip implants, knee implants etc. extending to their detailed specifications such as diameter, lengths etc.
              4. Clinical queries regarding intricate workflow and procedural steps. 
              This system should have specific responses especially when it interfaces with implant dimensions and inventory management. Will this approach be mostly beneficial in 'what' and 'where' case situations amongst others? 
              Your capabilities include providing information about this context/Information :{context}""")
    system_message_prompt = SystemMessagePromptTemplate.from_template(template)
    human_template = "{inquiry}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )

    # Asking about 'knee implants' for this instance
    chatgpt_result=chat(
        chat_prompt.format_prompt(
            context=api_source, inquiry=question
        ).to_messages()
    )
    print(chatgpt_result.content)
    return chatgpt_result.content

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)
